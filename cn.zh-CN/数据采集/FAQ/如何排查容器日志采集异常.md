# 排查容器日志采集异常

当您使用Logtail采集容器（标准容器、Kubernetes）的日志时，如果采集状态异常，可以根据本文进行排查问题、检查运行状态等运维操作。

## 排查异常

-   [排查机器组心跳异常](#section_azl_rfh_1fb)
-   [排查容器日志采集异常](#section_cps_vch_1fb)

## 其他运维操作

-   [登录Logtail容器](#section_hg2_hsb_dfb)
-   [查看Logtail的运行日志](#section_frc_wjh_1fb)
-   [Logtail的容器标准输出（stdout）](#section_g1k_5sb_dfb)
-   [查看Kubernetes集群中日志相关组件状态](#section_xls_blh_1fb)
-   [查看Logtail的版本号信息、IP地址和时间](#section_f52_2lh_1fb)
-   [误删CRD创建出的Logstore后，应如何处理](#section_uwm_hlh_1fb)

## 排查机器组心跳异常

您可以通过检查机器组心跳状态的方式判断容器上的Logtail是否已正确安装。

1.  查看机器组心跳状态。

    1.  登录[日志服务控制台](https://sls.console.aliyun.com)，单击Project名称。

    2.  登录[日志服务控制台](https://partners-intl.console.aliyun.com/#/sls)，单击Project名称。

    3.  在左侧导航栏中单击**Logtail机器组**。

    4.  找到要查看状态的机器组，并单击**查看状态**。

        记录心跳状态为**OK**的节点数。

2.  检查集群中Worker节点数。

    执行命令`kubectl get node | grep -v master`，查看集群中Worker节点数。

    ```
    $kubectl get node | grep -v master
    NAME                                 STATUS    ROLES     AGE       VERSION
    cn-hangzhou.i-bp17enxc2us3624wexh2   Ready     <none>    238d      v1.10.4
    cn-hangzhou.i-bp1ad2b02jtqd1shi2ut   Ready     <none>    220d      v1.10.4
    ```

3.  对比心跳状态为**OK**的节点数是否和集群中Worker节点数一致。根据对比结果选择排查方式。

    -   机器组中所有节点的心跳状态均为**Failed**。
        -   若您使用[采集标准Docker容器日志](/cn.zh-CN/数据采集/Logtail采集/采集容器日志/采集标准Docker容器日志.md)，请参见[参数说明](/cn.zh-CN/数据采集/Logtail采集/采集容器日志/采集标准Docker容器日志.md)检查$\{your\_region\_name\}、$\{your\_aliyun\_user\_id\}和$\{your\_machine\_group\_user\_defined\_id\}是否填写正确。
        -   若您使用[阿里云容器服务Kubernetes安装方式](/cn.zh-CN/数据采集/Logtail采集/采集容器日志/概述.md)，请提交工单至日志服务。
        -   若您使用[自建Kubernetes安装方式](/cn.zh-CN/数据采集/Logtail采集/采集容器日志/概述.md)，请参见[参数说明](/cn.zh-CN/数据采集/Logtail采集/采集容器日志/概述.md)，检查\{your-project-suffix\}、\{regionId\}、\{aliuid\}、\{access-key-id\}和\{access-key-secret\}是否已正确填写，若填写错误请执行命令`helm del --purge alibaba-log-controller`删除安装包，并重新安装。
    -   机器组心跳数少的节点于集群中Worker节点数。

        1.  判断是否使用yaml文件手动部署DaemonSet。

            执行命令`kubectl get po -n kube-system -l k8s-app=logtail`，若有返回结果，表示您之前使用yaml文件手动部署了DaemonSet。

        2.  下载最新版本的[DaemonSet模板](http://logtail-release.oss-cn-hangzhou.aliyuncs.com/docker/k8s/logtail-daemonset.yaml)。
        3.  将其中的$\{your\_region\_name\}、$\{your\_aliyun\_user\_id\}、$\{your\_machine\_group\_name\}替换为您的真实内容。
        4.  执行命令`kubectl apply -f ./logtail-daemonset.yaml`更新DaemonSet yaml文件。
        其他情况，请提交工单至日志服务。


## 排查容器日志采集异常

如果您在控制台的预览或查询页面未查看到日志数据，说明日志服务并未采集到您的容器日志数据。请确认容器状态后，执行以下检查。

1.  [确认机器组状态](#section_azl_rfh_1fb)是否正常。

2.  检查采集配置标识是否正确。

    检查配置中IncludeLabel、ExcludeLabel、IncludeEnv、ExcludeEnv是否和您需要采集的容器配置匹配。

    **说明：** 其中的`Label`为容器Label（docker inspect中的Label信息），并不是Kubernetes中定义的Label。 您可以通过将IncludeLabel、ExcludeLabel、IncludeEnv和ExcludeEnv配置临时去除，查看是否可以正常采集到日志，若可以采集则可以确定为标记配置问题。

3.  检查其他注意事项。

    若您配置采集容器内文件，请注意：

    -   若下发采集配置后文件没有修改事件，Logtail则不采集。
    -   采集容器日志文件，只支持采集容器默认存储或挂载到本地的文件，暂不支持其他存储方式。

## 登录Logtail容器

-   普通Docker

    1.  在宿主机执行`docker ps | grep logtail`搜索Logtail容器。
    2.  执行`docker exec -it ****** bash`登录。
    ```
    $docker ps | grep logtail
    223fbd3ed2a6e        registry.cn-hangzhou.aliyuncs.com/log-service/logtail                             "/usr/local/ilogta..."   8 days ago          Up 8 days                               logtail-iba
    $docker exec -it 223fbd3ed2a6e bash
    ```

-   Kubernetes

    1.  执行`kubectl get po -n kube-system | grep logtail`搜索Logtail的Pod。
    2.  执行`kubectl exec -it -n kube-system ****** bash`登录Pod。
    ```
    $kubectl get po -n kube-system | grep logtail
    logtail-ds-g5wgd                                             1/1       Running    0          8d
    logtail-ds-slpn8                                             1/1       Running    0          8d
    $kubectl exec -it -n kube-system logtail-ds-g5wgd bash
    ```


## 查看Logtail的运行日志

Logtail日志存储在Logtail容器中的/usr/local/ilogtail/目录中，文件名为ilogtail.LOG和logtail\_plugin.LOG。

1.  [登录Logtail容器](#section_hg2_hsb_dfb)。

2.  打开Logtail容器目录/usr/local/ilogtail/。

    ```
    cd /usr/local/ilogtail
    ```

3.  查看文件ilogtail.LOG和logtail\_plugin.LOG。

    ```
    cat ilogtail.LOG
    cat logtail_plugin.LOG
    ```


## Logtail的容器标准输出（stdout）

容器stdout并不具备参考意义，请忽略以下标准输出（stdout）：

```
start umount useless mount points, /shm$|/merged$|/mqueue$
umount: /logtail_host/var/lib/docker/overlay2/3fd0043af174cb0273c3c7869500fbe2bdb95d13b1e110172ef57fe840c82155/merged: must be superuser to unmount
umount: /logtail_host/var/lib/docker/overlay2/d5b10aa19399992755de1f85d25009528daa749c1bf8c16edff44beab6e69718/merged: must be superuser to unmount
umount: /logtail_host/var/lib/docker/overlay2/5c3125daddacedec29df72ad0c52fac800cd56c6e880dc4e8a640b1e16c22dbe/merged: must be superuser to unmount
......
xargs: umount: exited with status 255; aborting
umount done
start logtail
ilogtail is running
logtail status:
ilogtail is running
```

## 查看Kubernetes集群中日志相关组件状态

执行命令`helm status alibaba-log-controller`可以查看Kubernetes集群中日志相关组件状态。

## 查看Logtail的版本号信息、IP地址、启动时间

相关信息存储在Logtail容器中的/usr/local/ilogtail/目录中的app\_info.json，示例如下：

```
kubectl exec logtail-ds-gb92k -n kube-system cat /usr/local/ilogtail/app_info.json
{
   "UUID" : "",
   "hostname" : "logtail-gb92k",
   "instance_id" : "0EBB2B0E-0A3B-11E8-B0CE-0A58AC140402_172.20.4.2_1517810940",
   "ip" : "172.20.4.2",
   "logtail_version" : "0.16.2",
   "os" : "Linux; 3.10.0-693.2.2.el7.x86_64; #1 SMP Tue Sep 12 22:26:13 UTC 2017; x86_64",
   "update_time" : "2018-02-05 06:09:01"
}
```

## 误删CRD创建出的Logstore后，应如何处理

若您删除了CRD自动创建出的Logstore，已采集的数据无法恢复，并且针对此Logstore的CRD配置会失效，您可以选择以下方案避免日志采集异常：

-   在CRD配置中使用其他Logstore，避免使用手动误删的Logstore名。
-   重新启动POD alibaba-log-controller。该POD可通过命令`kubectl get po -n kube-system | grep alibaba-log-controller`查找。

